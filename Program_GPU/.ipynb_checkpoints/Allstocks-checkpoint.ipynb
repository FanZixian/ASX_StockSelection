{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock Selection Machine learning\n",
    "## Targets: \n",
    "1. Train 4 models to achieve the accuracy target\n",
    "2. Backtesting the models to calculate profits generated both from stock price and dividend yield\n",
    "3. Consider dividend yield during model training, and make a comparison with the previous results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Packages imports and initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basics and visualization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "import json\n",
    "from numpy import vstack\n",
    "\n",
    "# Data Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, QuantileTransformer\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.utils import resample\n",
    "import os\n",
    "from sklearn import model_selection\n",
    "\n",
    "# Models to be considered\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "from torch.optim import SGD\n",
    "from torch.nn.init import xavier_uniform_\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, roc_auc_score\n",
    "from sklearn.metrics import mean_squared_error as MSE, r2_score, mean_absolute_error\n",
    "from torch.nn import MSELoss\n",
    "from math import sqrt\n",
    "\n",
    "# Warning ignorance\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"E:/NUS_Exchange/Research/Datasets/\"\n",
    "try:\n",
    "    os.mkdir(path + \"Output\")\n",
    "    os.mkdir(path + \"Data\")\n",
    "    os.mkdir(path + \"Output/GridSearchCV_Result\")\n",
    "    os.mkdir(path + \"Output/HoldOutValidation\")\n",
    "except:\n",
    "    print(\"Folder already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading csv files from local repository\n",
    "def readX(pathname):\n",
    "    '''\n",
    "    Read X related file\n",
    "    '''\n",
    "    df = pd.read_csv(pathname, index_col=[0], header=[0])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_dir(file_dir):\n",
    "    # list_csv = []\n",
    "    dir_list = os.listdir(file_dir)\n",
    "    for cur_file in dir_list:\n",
    "        path = os.path.join(file_dir,cur_file)\n",
    "        if os.path.isfile(path):\n",
    "            # print(\"{0} : is file!\".format(cur_file))\n",
    "            dir_files = os.path.join(file_dir, cur_file)\n",
    "        if os.path.splitext(path)[1] == '.csv':\n",
    "            csv_file = os.path.join(file_dir, cur_file)\n",
    "            # print(os.path.join(file_dir, cur_file))\n",
    "            # print(csv_file)\n",
    "            list_csv.append(csv_file)\n",
    "        if os.path.isdir(path):\n",
    "            # print(\"{0} : is dir\".format(cur_file))\n",
    "            # print(os.path.join(file_dir, cur_file))\n",
    "            list_dir(path)\n",
    "    return list_csv\n",
    " \n",
    " \n",
    "if __name__ == '__main__':\n",
    "    paths = r'E:/NUS_Exchange/Research/Datasets/Data/ASX_stockconsidering'\n",
    "    list_csv = []\n",
    "    list_dir(file_dir=paths)\n",
    "    print(list_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Stock price data scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use one stock as an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AIA_df = readX(list_csv[0])\n",
    "AIA_df['Date'] = pd.to_datetime(AIA_df.index)\n",
    "AIA_df = AIA_df.set_index(['Date'], drop=True)\n",
    "\n",
    "split_date1 = pd.Timestamp('2017-01-03')\n",
    "split_date2 = pd.Timestamp('2019-11-05')\n",
    "split_date3 = pd.Timestamp('2021-04-09')\n",
    "split_date4 = pd.Timestamp('2021-04-12')\n",
    "split_date5 = pd.Timestamp('2022-09-08')\n",
    "\n",
    "TV_AIA_df = AIA_df.loc[split_date1:split_date3]\n",
    "Test_AIA_df = AIA_df.loc[split_date4:split_date5]\n",
    "\n",
    "# Didn't include dividend and split\n",
    "X_train = TV_AIA_df[[\"Volume\", \"OpenClose_spread\", \"Highlow_spread\", \"5_Days_MA\", \"10_Days_MA\", \"15_Days_MA\", \"30_Days_MA\", \"5_Days_VAR\", \"15_Days_VAR\", \"30_Days_VAR\", \n",
    "                        \"15_Days_EWMA\", \"15_Days_RSI\", \"15_Days_MFI\", \"15_Days_ATR\", \"ForceIndex\", \"Typical_MACD\"]]\n",
    "X_train_dividend = TV_AIA_df[[\"Dividends\"]]\n",
    "X_test = Test_AIA_df[[\"Volume\", \"OpenClose_spread\", \"Highlow_spread\", \"5_Days_MA\", \"10_Days_MA\", \"15_Days_MA\", \"30_Days_MA\", \"5_Days_VAR\", \"15_Days_VAR\", \"30_Days_VAR\", \n",
    "                        \"15_Days_EWMA\", \"15_Days_RSI\", \"15_Days_MFI\", \"15_Days_ATR\", \"ForceIndex\", \"Typical_MACD\"]]\n",
    "X_test_dividend = Test_AIA_df[[\"Dividends\"]]\n",
    "Y_train = TV_AIA_df[[\"Price\"]]\n",
    "Y_test = Test_AIA_df[[\"Price\"]]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = Y_train['Price'].plot()\n",
    "Y_test['Price'].plot(ax=ax)\n",
    "plt.legend(['train', 'validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scales = [StandardScaler(), MinMaxScaler(), QuantileTransformer()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test and Criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getScore(pipe_lr, X_valid, y_valid, prob = False):\n",
    "    '''\n",
    "    Get ROC\n",
    "    '''\n",
    "    y_pred = pipe_lr.predict(X_valid)\n",
    "    if prob:\n",
    "        y_pred = pipe_lr.predict_proba(X_valid)[:,1]\n",
    "    print('ROC: %.4f' % roc_auc_score(y_true=y_valid, y_score=y_pred))\n",
    "\n",
    "def evaluate_model(test_dl, model):\n",
    "    predictions, actuals = list(), list()\n",
    "    for i, (inputs, targets) in enumerate(test_dl):\n",
    "        # evaluate the model on the test set\n",
    "        yhat = model(inputs)\n",
    "        # retrieve numpy array\n",
    "        yhat = yhat.detach().numpy()\n",
    "        actual = targets.numpy()\n",
    "        actual = actual.reshape((len(actual), 1))\n",
    "        # store\n",
    "        predictions.append(yhat)\n",
    "        actuals.append(actual)\n",
    "    predictions, actuals = vstack(predictions), vstack(actuals)\n",
    "    # calculate mse\n",
    "    mse = MSE(actuals, predictions)\n",
    "    return mse\n",
    "\n",
    "def getPrediction_ANN(predict_dl, model):\n",
    "    predictions = list()\n",
    "    for i, (inputs, targets) in enumerate(predict_dl):\n",
    "        # evaluate the model on the test set\n",
    "        yhat = model(inputs)\n",
    "        # retrieve numpy array\n",
    "        yhat = yhat.detach().numpy()\n",
    "        predictions.append(yhat)\n",
    "    predictions = vstack(predictions)\n",
    "    \n",
    "    return pd.DataFrame(data=predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rolling windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mse = []\n",
    "Totaldatas = 720\n",
    "Window = 30\n",
    "Rollingtimes = 12\n",
    "\n",
    "def RollingTrain(X_train, Y_train, times, selectedModel, scale):\n",
    "    X1 = X_train.iloc[Window * times : Totaldatas + Window * times]\n",
    "    Y1 = Y_train.iloc[Window * times : Totaldatas + Window * times]\n",
    "    X1_ = X_train.iloc[Totaldatas + Window * times : Totaldatas + Window * (times + 1)]\n",
    "    Y1_ = Y_train.iloc[Totaldatas + Window * times : Totaldatas + Window * (times + 1)]\n",
    "\n",
    "    scaler = scale.fit(X1)\n",
    "    X1_scaled = scaler.transform(X1)\n",
    "    X1__scaled = scaler.fit_transform(X1_)\n",
    "    model = selectedModel.fit(X1_scaled, Y1)\n",
    "    prediction = model.predict(X1__scaled)\n",
    "    mse = MSE(Y1_, prediction)\n",
    "    model_mse.append(mse)\n",
    "\n",
    "def TrainRolling(Rollingtimes, selectedModel, scale):\n",
    "    for i in range(0, Rollingtimes):\n",
    "        RollingTrain(X_train, Y_train, i, selectedModel, scale)\n",
    "    return np.mean(model_mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Parameters to be considered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_n_estimators = [50, 100, 300, 500, 800, 1200]\n",
    "rf_max_depth = [None, 20, 100, 300, 500, 1000]\n",
    "rf_max_leaf_nodes = [1000, 500, 200, 100]\n",
    "rf_min_samples_split = [2, 10, 20, 50]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_parameters = []\n",
    "def RandomForestTraining():\n",
    "    averageMSE = []\n",
    "    for n_estimators in rf_n_estimators:\n",
    "        for max_depth in rf_max_depth:\n",
    "            for max_leaf_nodes in rf_max_leaf_nodes:\n",
    "                for min_samples_split in rf_min_samples_split:\n",
    "                    for scaler in scales:\n",
    "                        rf = RandomForestRegressor(n_estimators = n_estimators, max_depth = max_depth, max_leaf_nodes = max_leaf_nodes, min_samples_split = min_samples_split)\n",
    "                        averageMSE.append(TrainRolling(Rollingtimes, rf, scaler))\n",
    "                        rf_parameters.append([\"rf_n_estimators: \", n_estimators, \"; rf_max_depth:\", max_depth, \"; rf_max_leaf_nodes: \", max_leaf_nodes, \"; rf_min_samples_split: \", \n",
    "                                             min_samples_split, \"; scaler: \", scaler])\n",
    "    return averageMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = RandomForestTraining()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "3cd2506ed87cf616db555091a1f51ea367a230f8881c6d5beb7cb815d220d913"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
